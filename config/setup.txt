#!/bin/bash -i

apt-get update
apt-get -y upgrade
apt install openjdk-8-jdk -y scala python3-pip -y
pip install --no-cache-dir pyspark==2.1.2

#Install hadoop 
wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.4/hadoop-3.3.4.tar.gz
tar -xzvf hadoop-3.3.4.tar.gz
mv hadoop-3.3.4 /usr/local/hadoop
rm -f *.gz

#Install spark 
wget https://archive.apache.org/dist/spark/spark-2.2.0/spark-2.2.0-bin-without-hadoop.tgz
tar -xzvf spark-2.2.0-bin-without-hadoop.tgz
mv spark-2.2.0-bin-without-hadoop /opt/spark
rm -f *.tgz

# Setup hadoop config
cat <<EOF > /usr/local/hadoop/etc/hadoop/hadoop-env.sh
export JAVA_HOME=$(readlink -f /usr/bin/java | sed "s:bin/java::")
export HADOOP_HOME=/usr/local/hadoop
EOF

# Set env variables  
echo export JAVA_HOME=$(readlink -f /usr/bin/java | sed "s:bin/java::") >> ~/.profile
echo export HADOOP_HOME=/usr/local/hadoop >> ~/.profile
echo export SPARK_HOME=/opt/spark >> ~/.profile
echo export PYSPARK_PYTHON=/usr/bin/python3 >> ~/.profile
echo export PATH=$PATH:/opt/spark/bin:/opt/spark/sbin:/usr/local/hadoop/bin >> ~/.profile
source ~/.profile
echo export SPARK_DIST_CLASSPATH=$(hadoop classpath) >> ~/.profile
source ~/.profile

# Start spark
start-master.sh
